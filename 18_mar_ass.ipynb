{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8b0d8-4203-4f47-904d-fe28738943d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q1. What is the Filter method in feature selection, and how does it work?\n",
    "AnswerFilter methods use statistical calculation to evaluate the relevance of the predictors outside of the predictive\n",
    "models and keep only the predictors that pass some criterion. [2] Considerations when choosing filter methods are the \n",
    "types of data involved, both in predictors and outcome — either numerical or categorical.\n",
    "\n",
    "Working of filter method:\n",
    "Filter method in feature engineering find the correlation between the features and select the only relevent feature that can \n",
    "highly effect the model accuracy.\n",
    "Because larger no. of feature can reduce the accuracy of our model.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80846879-623d-43fc-b3ea-dc4802920c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
    "Answer-Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods\n",
    "measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared to\n",
    "wrapper methods as they do not involve training the model.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f1b9d-cb69-422a-93f1-08fc09b6d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q3. What are some common techniques used in Embedded feature selection methods?\n",
    "Answer-Embedded methods are models where the feature selection procedure occurs naturally in the course of the model fitting\n",
    "process. \n",
    "Put simply, this method integrates the feature selection algorithm as part of the machine learning algorithm. \n",
    "\n",
    "The most typical embedded technique is tree based algorithm, which includes decision tree and random forest. The general \n",
    "idea of feature selection is decided at the splitting node based on information gain. Other exemplars of embedded methods\n",
    "are the LASSO with the L1 penalty and Ridge with the L2 penalty for constructing a linear model.\n",
    "\n",
    "The Common ML techniques used for embedded platforms include SVMs (Support Vector Machine),\n",
    "CNNs (convolutional neural network), DNNs (Deep Neural networks), k-NNs (K-Nearest Neighbour), and Naive Bayes'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58082139-0c96-42cc-9fd7-1c61d70b1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q4. What are some drawbacks of using the Filter method for feature selection?\n",
    "Answer-The common disadvantage of filter methods is that they ignore the interaction with the classifier and each feature\n",
    "is considered independently thus ignoring feature dependencies In addition, it is not clear how to determine the threshold \n",
    "point for rankings to select only the required features and exclude noise.\n",
    "Another disadvantage of filter method in feature selection is that it can be cause of loss important feature or data that \n",
    "is related to our model prediction'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fab68-ac3e-49e2-8fd7-a002968ac00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?\n",
    "Answer-This approach selects a subset of features without using a learning algorithm. It is used in many datasets where the \n",
    "number of features is high. Filter-based feature selection methods are faster than wrapper-based methods.\n",
    "Filter method is fast and less complex in the comparison of wrapper methods.\n",
    "In some of the cases wrapper method lead to overfit the model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e7d44-3b0c-47ef-a131-590e228edd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n",
    "Answer\n",
    "The Filter Method is one of the feature selection techniques used to identify the most relevant features from\n",
    "a dataset based on statistical tests or other metrics. Here are the steps to choose the most pertinent attributes for\n",
    "a customer churn prediction model using the Filter Method:\n",
    "\n",
    "1. Define the Target Variable: The first step is to define the target variable, which is customer churn in this case.\n",
    "The target variable will be used to evaluate the significance of each feature and its relationship with the target variable.\n",
    "\n",
    "2. Select the Feature Metrics: The next step is to select the feature metrics to measure the relevance of each feature.\n",
    "Some common metrics used in the Filter Method include:\n",
    "\n",
    "3. Pearson Correlation: measures the linear relationship between two variables, with values ranging from -1 to 1.\n",
    "Chi-Square: measures the dependence between two categorical variables.\n",
    "ANOVA F-value: measures the difference in means between two or more groups.\n",
    "Compute the Feature Metrics: For each feature, calculate the selected feature metrics with respect to the target variable.\n",
    "\n",
    "4. Rank the Features: Rank the features based on their respective metrics, from highest to lowest. The higher the metric\n",
    "value, the more relevant the feature.\n",
    "\n",
    "5. Select the Relevant Features: Finally, select the most relevant features based on the desired number of features or \n",
    "the threshold value of the metric.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960228b-7d1c-4ea9-903d-cebbf75e40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.\n",
    "Answer-Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their \n",
    "own built-in feature selection methods.\n",
    "\n",
    "Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization\n",
    "functions to reduce overfitting.\n",
    "\n",
    "Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "\n",
    "Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aede65-31dc-4b71-83bf-823d9427d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor.\n",
    "Answer-In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that\n",
    "we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially\n",
    "reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature \n",
    "elimination, etc.\n",
    "\n",
    "Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each\n",
    "iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the\n",
    "performance of the model.\n",
    "\n",
    "By this technique we select one by one feature and calculate the accuracy of the model and remove that feature which is not\n",
    "incresing the accuracy of the model\n",
    "Like first we take feature like size and the loaction and if age is increasing the accuracy of the model then we \n",
    "\n",
    "Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at \n",
    "each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of \n",
    "features.\n",
    "\n",
    "By Backword elimination technique first we select all the feature like size,location and age and then remove irelevent feature\n",
    "for the model.\n",
    "\n",
    "Recursive Feature elimination: It is a greedy optimization algorithm which aims to find the best performing feature subset. It\n",
    "repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next\n",
    "model with the left features until all the features are exhausted. It then ranks the features based on the order of their\n",
    "elimination.\n",
    "One of the best ways for implementing feature selection with wrapper methods is to use Boruta package that finds the \n",
    "importance of a feature by creating shadow features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee6555-4138-4550-a71d-63455d3af594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
